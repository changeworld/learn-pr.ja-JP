![Docker のロゴ](../media/3-image1.PNG)

Docker は、アプリケーションを任意のホスト オペレーティング システム上で実行するために、サンドボックスにデプロイできるツールです。 これを使用すると、アプリとすべての依存関係を標準化されたユニットにパッケージ化することができます。 しかし、DSVM ベース イメージには最も一般的なディープ ラーニング フレームワークが既に事前インストールされているとすれば、Docker を使用する理由は何でしょうか。

開発者はディープ ラーニング タスクを実行しようとすると、次のような依存関係の問題に直面します。 例: 

- カスタム パッケージ作成の必要性 - ディープ ラーニングの研究者は、GitHub にコードを発行するときに運用環境についてあまり考慮しない傾向があります。 研究者は、自分の開発環境でパッケージが動けば、通常、他のユーザーもできるものと単純に思い込みます。
- GPU ドライバーのバージョン管理 - CUDA は、NVIDIA によって開発された並列コンピューティング プラットフォームおよびアプリケーション プログラミング インターフェイス (API) です。 開発者はそれにより、CUDA 対応のグラフィックス処理装置 (GPU) を汎用処理に使用することができます。 Tensorflow の特定のバージョンは、CUDA 9.1 より後のバージョンでは機能しません。 PyTorch などの他のフレームワークは、CUDA の比較的新しいバージョンでパフォーマンスが向上するように見受けられます。

これらの問題を回避し、コードの使いやすさを向上させるには、Docker またはその GPU バリエーションの NVIDIA Docker を使用して、ディープ ラーニング プロジェクトを管理および実行できます。 

<!--Quiz 
What is CUDA? 
What versioning issues do deep learning engineers deal with? -->