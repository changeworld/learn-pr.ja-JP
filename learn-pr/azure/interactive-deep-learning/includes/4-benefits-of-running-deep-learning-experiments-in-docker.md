![Docker のロゴ](../media/3-image1.PNG)

<span data-ttu-id="14212-102">Docker は、アプリケーションを任意のホスト オペレーティング システム上で実行するために、サンドボックスにデプロイできるツールです。</span><span class="sxs-lookup"><span data-stu-id="14212-102">Docker is a tool that allows you to deploy your applications in a sandbox to run on a host operating system of your choice.</span></span> <span data-ttu-id="14212-103">これを使用すると、アプリとすべての依存関係を標準化されたユニットにパッケージ化することができます。</span><span class="sxs-lookup"><span data-stu-id="14212-103">It allows you to package your app with all of its dependencies in a standardized unit.</span></span> <span data-ttu-id="14212-104">しかし、DSVM ベース イメージには最も一般的なディープ ラーニング フレームワークが既に事前インストールされているとすれば、Docker を使用する理由は何でしょうか。</span><span class="sxs-lookup"><span data-stu-id="14212-104">But if the DSVM base image comes with the most popular deep learning frameworks already pre-installed, why would you use Docker?</span></span>

<span data-ttu-id="14212-105">開発者はディープ ラーニング タスクを実行しようとすると、次のような依存関係の問題に直面します。</span><span class="sxs-lookup"><span data-stu-id="14212-105">When attempting to run deep learning tasks, developers find themselves facing dependency issues.</span></span> <span data-ttu-id="14212-106">例:</span><span class="sxs-lookup"><span data-stu-id="14212-106">For example:</span></span> 

- <span data-ttu-id="14212-107">カスタム パッケージ作成の必要性 - ディープ ラーニングの研究者は、GitHub にコードを発行するときに運用環境についてあまり考慮しない傾向があります。</span><span class="sxs-lookup"><span data-stu-id="14212-107">Having to build custom packages - Deep learning researchers tend to think less about production when they publish code to GitHub.</span></span> <span data-ttu-id="14212-108">研究者は、自分の開発環境でパッケージが動けば、通常、他のユーザーもできるものと単純に思い込みます。</span><span class="sxs-lookup"><span data-stu-id="14212-108">If they can get a package working on their own development environment, they often just assume that others will be able to do so as well.</span></span>
- <span data-ttu-id="14212-109">GPU ドライバーのバージョン管理 - CUDA は、NVIDIA によって開発された並列コンピューティング プラットフォームおよびアプリケーション プログラミング インターフェイス (API) です。</span><span class="sxs-lookup"><span data-stu-id="14212-109">GPU driver versioning - CUDA is a parallel computing platform and application programming interface (API) developed by NVIDIA.</span></span> <span data-ttu-id="14212-110">開発者はそれにより、CUDA 対応のグラフィックス処理装置 (GPU) を汎用処理に使用することができます。</span><span class="sxs-lookup"><span data-stu-id="14212-110">It allows developers to use a CUDA-enabled graphics processing unit (GPU) for general-purpose processing.</span></span> <span data-ttu-id="14212-111">Tensorflow の特定のバージョンは、CUDA 9.1 より後のバージョンでは機能しません。</span><span class="sxs-lookup"><span data-stu-id="14212-111">Certain versions of Tensorflow will not work with versions of CUDA above 9.1.</span></span> <span data-ttu-id="14212-112">PyTorch などの他のフレームワークは、CUDA の比較的新しいバージョンでパフォーマンスが向上するように見受けられます。</span><span class="sxs-lookup"><span data-stu-id="14212-112">Other frameworks, such as PyTorch, seem to perform better with later versions of CUDA.</span></span>

<span data-ttu-id="14212-113">これらの問題を回避し、コードの使いやすさを向上させるには、Docker またはその GPU バリエーションの NVIDIA Docker を使用して、ディープ ラーニング プロジェクトを管理および実行できます。</span><span class="sxs-lookup"><span data-stu-id="14212-113">To get around these issues and to increase the usability of code, you can use Docker or its GPU variant NVIDIA Docker to manage and run deep learning projects.</span></span> 

<!--Quiz 
What is CUDA? 
What versioning issues do deep learning engineers deal with? -->