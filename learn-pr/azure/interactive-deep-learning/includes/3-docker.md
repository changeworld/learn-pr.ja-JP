## <a name="docker"></a>Docker

![Docker のロゴ](../media/3-image1.PNG)

Docker を使用すると、開発者は、ほとんどどこでも実行できる軽量で、ポータブルな、自己完結型のコンテナーとしてアプリケーションを簡単にパック、出荷、実行できます。 DSVM の基本イメージに最も一般的なディープ ラーニング フレームワークが事前インストールされている場合、Docker などのコンテナー化クライアントが必要なのはなぜでしょうか。

多くの場合、開発者はディープ ラーニング タスクを実行しようとすると、次のような依存関係の悪夢に直面します。 

- カスタム パッケージ作成の必要性 - ディープ ラーニングの研究者は、Git Hub にコードを発行するときに運用環境についてあまり考えない傾向があります。 研究者は、自分の開発環境でパッケージが動けば、通常、他のユーザーもできるものと単純に思い込みます。
- GPU ドライバーのバージョン管理 - CUDA は、Nvidia によって開発された並列コンピューティング プラットフォームおよびアプリケーション プログラミング インターフェイス (API) です。 ソフトウェア開発者やソフトウェア エンジニアはそれにより、CUDA 対応のグラフィックス処理装置 (GPU) を汎用処理に使用することができます。 特定のバージョンの Tensorflow は 9.1 より後の CUDA のバージョンでは動作しませんが、PyTorch などの他のフレームワークでは新しいバージョンの CUDA の方がパフォーマンスが向上するようです。

これらの問題を回避し、コードの使いやすさを向上させるには、Docker またはその GPU バリエーションの Nvidia-Docker を使用して、ディープ ラーニング プロジェクトを管理および実行できます。 

<!--Quiz 
What is CUDA? 
What versioning issues do deep learning engineers deal with? -->